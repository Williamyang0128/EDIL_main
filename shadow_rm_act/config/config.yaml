robot_env: {
  # TODO change the path to the correct one
  rm_left_arm: 'PATH/TO/YOUR/rm_left_arm.yaml',
  rm_right_arm: 'PATH/TO/YOUR/rm_right_arm.yaml',
  arm_axis: 6,
  head_camera: 'your_number',
  left_camera: 'your_number',
  bottom_camera: 'your_number',
  right_camera: 'your_number',
  init_left_arm_angle: [...],
  init_right_arm_angle: [...]
}
dataset_dir: ''PATH/TO/YOUR/DATASET'
checkpoint_dir: ''PATH/TO/YOUR/Checkpoint'
checkpoint_name: 'xxx.ckpt'
state_dim: 14
save_episode: True
num_rollouts: 3
real_robot: True
policy_class: 'MMArmPolicy'
onscreen_render: False
camera_names: ['cam_high', 'cam_low', 'cam_left', 'cam_right']
episode_len: 350(YOUR OWN LEN)
task_name: 'YOUR_TASK_NAME'
temporal_agg: False
batch_size: 12
seed: 500
chunk_size: 30 
eval_every: 1
num_steps: 2000
validate_every: 1
save_every: 500 
load_pretrain: False
resume_ckpt_path: 
name_filter:  # TODO
skip_mirrored_data: False 
stats_dir:  
sample_weights:  
train_ratio: 0.8 

policy_config: {
  hidden_dim: ...
  state_dim: ...
  position_embedding: 'sine', # ('sine', 'learned').Type of positional embedding to use on top of the image features
  lr_backbone: 1.0e-5,
  masks: False, # If true, the model masks the non-visible pixels
  backbone: 'resnet18',
  dilation: False, # If true, we replace stride with dilation in the last convolutional block (DC5)
  dropout: 0.1, # Dropout applied in the transformer
  nheads: 8,
  dim_feedforward: ..., # Intermediate size of the feedforward layers in the transformer blocks
  enc_layers: ..., # Number of encoding layers in the transformer
  dec_layers: ..., # Number of decoding layers in the transformer
  pre_norm: False, # If true, apply LayerNorm to the input instead of the output of the MultiheadAttention and FeedForward
  num_queries: 30,
  camera_names: ['cam_high', 'cam_low', 'cam_left', 'cam_right'],
  vq: False,
  vq_class: none,
  vq_dim: 64,
  action_dim: 14,
  no_encoder: False,
  lr: 1.0e-5,
  weight_decay: 1.0e-4,
  kl_weight: 10,

  # lr_drop: 200,
  # clip_max_norm: 0.1,
}



